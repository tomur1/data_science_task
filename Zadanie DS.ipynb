{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEMANTIVE: DATA SCIENTIST RECRUITMENT TASK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Please download and load the  [abalone dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/).  \n",
    "\n",
    "##### You can use information from [this](https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.names) file to add the proper headers to the columns. \n",
    "\n",
    "The whole task will be driven by supervised learning problem.\n",
    "Let's define target variable as $Rings / 1.5$ (it rougly corresponds to abalone's age).  \n",
    "We strongly encourage you use scikit-learn for the modeling tasks (but feel free to use different tools if you think they are appropriate).  \n",
    "\n",
    "##### First 2 tasks are obligatory. From the tasks 3-5 you can pick and complete 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A place for the imports\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.cluster import KMeans,MeanShift\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 1. Data exploration\n",
    "\n",
    "##### Explore the data  and provide a short summary of your findings. Pay attention to the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the data I can see 8 features and 1 target value which is the number of rings.\n",
    "#Data is saved in a CSV formatted file.\n",
    "#Sex values are a character, target values are an integer and the rest are floats/doubles.\n",
    "#There are 4177 entries in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 2. Supervised learning\n",
    "\n",
    "##### Prepare the data for the modeling.  \n",
    "\n",
    "###### Choose 2 supervised learning algorithms, that you think are suitable for this problem. Describe shortly how these algorithms work. Use them on the data and describe the results.  \n",
    "  \n",
    "Note. that we're more interested in comprehensive description and explanation of your choice than in model scores, so we don't expect you to tune your model yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge model predictions are: 42.9845143155875% correct\n",
      "K neares neighbours model predictions are: 26.035502958579883% correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomur\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:56: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "#Preparing the data\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\"\n",
    "\n",
    "#getting the data\n",
    "\n",
    "data = pd.read_csv(url)\n",
    "data = np.split(data,[8],1)\n",
    "\n",
    "features = data[0]\n",
    "labels = data[1]\n",
    "\n",
    "#Adding names to columns\n",
    "features.columns = ['Sex','Lenght','Diameter','Height','Whole weight','Shucked weight','Viscera weight','Shell weight']\n",
    "labels.columns = ['Rings']\n",
    "\n",
    "#using 0 for male, 1 for female and 2 for infant\n",
    "features['Sex'] = features['Sex'].map({'M' : 0 , 'F' : 1 , 'I' : 2})\n",
    "\n",
    "#converting to proper types for use in cross validation\n",
    "features = features.astype('float')\n",
    "labels = labels.astype('int')\n",
    "\n",
    "#Dividing data int training and test samples\n",
    "#samples from 0-3499 are for training and samples from 3500 to 4177 are for testing\n",
    "training_features = np.split(features,[3500])\n",
    "test_features = training_features[1]\n",
    "training_features = training_features[0]\n",
    "\n",
    "training_labels = np.split(labels,[3500])\n",
    "test_lables = training_labels[1]\n",
    "training_labels = training_labels[0]\n",
    "\n",
    "#converting data into vectors\n",
    "training_features_matrix = np.matrix(training_features).astype(np.float)\n",
    "training_labels_matrix = np.matrix(training_labels).astype(np.int)\n",
    "test_features_matrix = np.matrix(test_features).astype(np.float)\n",
    "test_lables_matrix = np.matrix(test_lables).astype(np.int)\n",
    "\n",
    "#As a first supervised algorythm I have chosen Ridge regression. It's a linear regression algorithm that tries to make a line\n",
    "#which tries to predict the target value given some features.\n",
    "#Thanks to the delta value it's less prone to overfitting training data than for example least squares regression\n",
    "#It also works well with a large number of features\n",
    "\n",
    "ridge_model = Ridge()\n",
    "ridge_model.fit(training_features_matrix,training_labels_matrix)\n",
    "score = ridge_model.score(test_features,test_lables)\n",
    "print('Ridge model predictions are: ' + repr(score*100) + '% correct')\n",
    "\n",
    "\n",
    "#Second supervised algorythm I used is K nearest neighbours\n",
    "#This time it's a classification algorithm, not a regression one. That means that now instead trying to predict a value\n",
    "#we try to predict a category.\n",
    "#That means that insted getting float point values like in Ridge, predictions will be integer values.\n",
    "\n",
    "neighbours = KNeighborsClassifier()\n",
    "neighbours.fit(training_features_matrix,training_labels_matrix)\n",
    "score_neighbours = neighbours.score(test_features_matrix,test_lables_matrix)\n",
    "print('K neares neighbours model predictions are: ' + repr(score_neighbours*100) + '% correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Other tasks - Pick 2 of them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3. Dimensionality reduction\n",
    "\n",
    "\n",
    "##### What are the applications of the dimensionality reduction?   \n",
    "\n",
    "###### Pick two algorithms that could be useful for exploration or supervised learning problem and apply them on the data. Describe what you've found. Provide a short description of the algorithms you've chosen.\n",
    "\n",
    "Note: feature selection is also one type of dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 4. Clustering\n",
    "\n",
    "##### What is clustering used for? Name the popular types of clustering. Pick two clustering algorithms and run them on the dataset. Describe what you've found. Does it help with the supervised learning task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering is an operation that tries to group individual pieces od data based on some characteristics.\n",
    "#It's usually a type of unsupervised learning so it doesn't require labels.\n",
    "\n",
    "#One of popular clustering algorithms is K-means clustering.\n",
    "#It works by choosing K (which is the number of groups) and then placing group center points randomly and assigning data to the\n",
    "#group which's center is the closest.\n",
    "#Then we recalculate the position of the centers so that it's in the mean of all datapoint that were assigned to this group\n",
    "\n",
    "means = KMeans()\n",
    "means.fit(training_features_matrix,training_labels_matrix)\n",
    "score_means = means.score(test_features_matrix,test_lables_matrix)\n",
    "\n",
    "#Another popular clustering algorithm is Mean-Shift.\n",
    "#It works by choosing some number of circles with radius R and then shifting them in the direction of most dense areas of data.\n",
    "#Thanks to this it will automaticly discover the number of groups.\n",
    "\n",
    "mean_shift = MeanShift()\n",
    "mean_shift.fit(training_features_matrix)\n",
    "predict_mean_shift = mean_shift.predict(test_features_matrix)\n",
    "\n",
    "#It helps in categorizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 5. Hyperparameter selection and crossvalidation\n",
    "\n",
    "##### We imagine that you did some modeling in 2nd task with the methods that have some tunable hyperparameters. If they don't, either find a versions of them that have that are tunable, or pick the different ones.\n",
    "\n",
    "##### Tune the hyperparameters of your model using cross-validation. Does it make it better? Does it solve overfitting problems? Is cross-validation score worse than score that your model achieves on test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression using cross validation results: [0.4031994  0.21838992 0.5156747  0.52751759 0.45207689] Mean of the scores is: 42.33717008551297%\n",
      "K neares neighbours using cross validation results: [0.19294118 0.2627824  0.22836538 0.24607961 0.25970874] Mean of the scores is: 23.797546296906198%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomur\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "C:\\Users\\tomur\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\tomur\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\tomur\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\tomur\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\tomur\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    }
   ],
   "source": [
    "#Score of my Ridge model without changing the deafult hyperparameters value is 42.9845143155875%\n",
    "#And score of K neares neighbours model is 26.035502958579883 %\n",
    "\n",
    "score_cross_ridge = cross_val_score(ridge_model,features,labels,cv=5)\n",
    "print('Ridge regression using cross validation results: {} Mean of the scores is: {}%'.format(score_cross_ridge,score_cross_ridge.mean()*100))\n",
    "#The mean of the scores is slightly worse than the result we got before, but some individual scores are much better for example\n",
    "#52.75%. The mean score of crossvalidation shows more accurately how the model would behave with different data samples.\n",
    "                                    \n",
    "score_cross_neighbours = cross_val_score(neighbours,features,labels,cv=5)\n",
    "print('K neares neighbours using cross validation results: {} Mean of the scores is: {}%'.format(score_cross_neighbours,score_cross_neighbours.mean()*100))\n",
    "#With K nearest neighbours algorythm the situation is simmilar. Some scores are higher than the previous score but the mean is lower.\n",
    "#Crossvalidation score is indeed lower than what model achieves on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
